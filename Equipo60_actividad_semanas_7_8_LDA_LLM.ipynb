{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "7jimvsiVgjMg",
        "Kx-dZSFJz9cK"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FranciscoMedellin/MNA_NLP/blob/main/Equipo60_actividad_semanas_7_8_LDA_LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Procesamiento de Lenguaje Natural**\n",
        "\n",
        "## Maestría en Inteligencia Artificial Aplicada\n",
        "#### Tecnológico de Monterrey\n",
        "#### Prof Luis Eduardo Falcón Morales\n",
        "\n",
        "### **Adtividad en Equipos Semanas 7 y 8 : LDA y LMM audio-a-texto**"
      ],
      "metadata": {
        "id": "-hVND8xY2OKY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **Nombres y matrículas:**\n",
        "\n",
        "  *   Nombre : Diego Andres Bernal Diaz <br> Matricula : A01795975\n",
        "  * Nombre : Francisco Medellin Zertuche <br> Matricula : A01794044\n",
        "  *   Nombre: Laura Patricia Martinez Treviño <br> Matricula:A01795967\n",
        "  *   Nombre : Mardonio Manuel Román Ramírez <br> Matricula : A01795265\n",
        "\n",
        "* **Número de Equipo: 60**\n"
      ],
      "metadata": {
        "id": "aimHVFOv23lm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* ##### **En cada ejercicio pueden importar los paquetes o librerías que requieran.**\n",
        "\n",
        "* ##### **En cada ejercicio pueden incluir las celdas y líneas de código que deseen.**"
      ],
      "metadata": {
        "id": "7jimvsiVgjMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "UTkUjdwwCd60",
        "outputId": "3ad83880-d23f-4759-dbd2-91cc39b12877"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai-whisper\n",
            "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.9.0)\n",
            "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=e3f0d8245e1d2752b23ad5c2e6577aa5c2ef7fe978417bd0afe51714b7156014\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa   # paquete para audio y música\n",
        "import warnings\n",
        "# para filtrar advertencias:\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "Pp5diad14yOj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-QUgzv1w8tyL",
        "outputId": "67871026-c5b4-47c0-b17b-40684987ed9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "DIR = \"/content/drive/MyDrive/8.NLP/Semana7/audios\"\n",
        "os.chdir(DIR)"
      ],
      "metadata": {
        "id": "k66dZiGq8yH-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim pyLDAvis"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "elAxRbLaVb5U",
        "outputId": "75b27bb0-2a0f-4be7-a621-7051c86d01ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting pyLDAvis\n",
            "  Downloading pyLDAvis-3.4.1-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (2.10.2)\n",
            "Collecting funcy (from pyLDAvis)\n",
            "  Downloading funcy-2.0-py2.py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis) (1.17.0)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m69.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyLDAvis-3.4.1-py3-none-any.whl (2.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m76.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading funcy-2.0-py2.py3-none-any.whl (30 kB)\n",
            "Installing collected packages: funcy, numpy, scipy, gensim, pyLDAvis\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.3\n",
            "    Uninstalling scipy-1.15.3:\n",
            "      Successfully uninstalled scipy-1.15.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.13.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed funcy-2.0 gensim-4.3.3 numpy-1.26.4 pyLDAvis-3.4.1 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y scipy numpy\n",
        "!pip install numpy==1.26.4 scipy==1.12.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9g9KM-mMWvgz",
        "outputId": "6b0ab240-2e30-4377-adff-8b0b6811bcd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: scipy 1.13.1\n",
            "Uninstalling scipy-1.13.1:\n",
            "  Successfully uninstalled scipy-1.13.1\n",
            "Found existing installation: numpy 1.26.4\n",
            "Uninstalling numpy-1.26.4:\n",
            "  Successfully uninstalled numpy-1.26.4\n",
            "Collecting numpy==1.26.4\n",
            "  Using cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Collecting scipy==1.12.0\n",
            "  Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.4/60.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "Downloading scipy-1.12.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.4/38.4 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "tsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4 scipy-1.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "!pip install -U numpy==1.24.4\n",
        "!pip install gensim==4.3.1 pyLDAvis==3.4.1"
      ],
      "metadata": {
        "id": "-xFpnt0A0Ub7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00733f72-0520-455c-ffd0-365d14691f5d",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy==1.24.4 in /usr/local/lib/python3.11/dist-packages (1.24.4)\n",
            "Collecting gensim==4.3.1\n",
            "  Downloading gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: pyLDAvis==3.4.1 in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (1.24.4)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (1.12.0)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim==4.3.1) (7.1.0)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (2.2.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (1.5.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (3.1.6)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (2.10.2)\n",
            "Requirement already satisfied: funcy in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (2.0)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (1.6.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from pyLDAvis==3.4.1) (75.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis==3.4.1) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis==3.4.1) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.0.0->pyLDAvis==3.4.1) (2025.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.0.0->pyLDAvis==3.4.1) (3.6.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim==4.3.1) (1.17.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->pyLDAvis==3.4.1) (3.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->pyLDAvis==3.4.1) (1.17.0)\n",
            "Downloading gensim-4.3.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.6/26.6 MB\u001b[0m \u001b[31m75.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 4.3.3\n",
            "    Uninstalling gensim-4.3.3:\n",
            "      Successfully uninstalled gensim-4.3.3\n",
            "Successfully installed gensim-4.3.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "import pyLDAvis.gensim_models\n",
        "\n",
        "print(\"¡Todo listo para usar LDA!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0C_7Ye-AMJ9",
        "outputId": "f8fbce93-e64c-43d3-9501-f4d2d41c44a4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "¡Todo listo para usar LDA!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 1:**"
      ],
      "metadata": {
        "id": "1BtP-Sk0DT-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Liga de los audios de las fábulas de Esopo:** https://www.gutenberg.org/ebooks/21144\n",
        "\n",
        "* #### **Descargar los 10 archivos de audio solicitados: 1, 4, 5, 6, 14, 22, 24, 25, 26, 27.**\n",
        "\n"
      ],
      "metadata": {
        "id": "Oh78pKeMghfe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos dos formas para acceder a los audios: <br>\n",
        "\n",
        "- La primera es que se guardaron en drive y desde alli se accedieron (la usada aqui).\n",
        "- La segunda usando la funcion descargar_mp3 tambien puede ser usada si asi se desea."
      ],
      "metadata": {
        "id": "Xt8o9rXCmLok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "\n",
        "def descargar_mp3(url, carpeta_destino=\"archivos\"):\n",
        "\n",
        "\n",
        "  try:\n",
        "    # Crear carpeta si no existe\n",
        "    if not os.path.exists(carpeta_destino):\n",
        "        os.makedirs(carpeta_destino)\n",
        "\n",
        "    # Obtener nombre del archivo de la URL\n",
        "    parsed_url = urlparse(url)\n",
        "    nombre_archivo = os.path.basename(parsed_url.path)\n",
        "\n",
        "    # Si no tiene extensión, agregar .mp3\n",
        "    if not nombre_archivo.endswith('.mp3'):\n",
        "        nombre_archivo += '.mp3'\n",
        "\n",
        "    ruta_completa = os.path.join(carpeta_destino, nombre_archivo)\n",
        "\n",
        "    # Descargar el archivo\n",
        "    print(f\"Descargando: {nombre_archivo}\")\n",
        "    response = requests.get(url, stream=True)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(ruta_completa, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8192):\n",
        "            f.write(chunk)\n",
        "\n",
        "    print(f\"✓ Descargado: {ruta_completa}\")\n",
        "    return True\n",
        "\n",
        "  except Exception as e:\n",
        "    print(f\"Error descargando {url}: {str(e)}\")\n",
        "    return False\n",
        "\n",
        "# Ejemplo de uso\n",
        "audio_solicitados = [1, 4, 5, 6, 14, 22, 24, 25, 26, 27]\n",
        "urls_mp3 = [f\"https://www.gutenberg.org/files/21144/mp3/21144-{x:02d}.mp3\" for x in audio_solicitados]\n",
        "\n",
        "for url in urls_mp3:\n",
        "    descargar_mp3(url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b7df49f6-ba86-4384-d1ed-3ba940a8260d",
        "id": "1pkNOSXBmG95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando: 21144-01.mp3\n",
            "✓ Descargado: archivos/21144-01.mp3\n",
            "Descargando: 21144-04.mp3\n",
            "✓ Descargado: archivos/21144-04.mp3\n",
            "Descargando: 21144-05.mp3\n",
            "✓ Descargado: archivos/21144-05.mp3\n",
            "Descargando: 21144-06.mp3\n",
            "✓ Descargado: archivos/21144-06.mp3\n",
            "Descargando: 21144-14.mp3\n",
            "✓ Descargado: archivos/21144-14.mp3\n",
            "Descargando: 21144-22.mp3\n",
            "✓ Descargado: archivos/21144-22.mp3\n",
            "Descargando: 21144-24.mp3\n",
            "✓ Descargado: archivos/21144-24.mp3\n",
            "Descargando: 21144-25.mp3\n",
            "✓ Descargado: archivos/21144-25.mp3\n",
            "Descargando: 21144-26.mp3\n",
            "✓ Descargado: archivos/21144-26.mp3\n",
            "Descargando: 21144-27.mp3\n",
            "✓ Descargado: archivos/21144-27.mp3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 2a:**"
      ],
      "metadata": {
        "id": "6uYgtCvvJSmq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Comenten el por qué del modelo seleccionado para extracción del texto de los audios.**\n",
        "\n",
        "* #### **Extraer el contenido de los audios en texto.**\n",
        "\n",
        "* #### **Sugerencia:** pueden extraerlo en un formato de diccionario, clave:valor $→$ {audio01:fabula01, ...}"
      ],
      "metadata": {
        "id": "rAQjVP2HkoZY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escogimos el Whisper de OpenAI - small, ya que, considerando la actividad es bueno para tareas generales y su velocidad es media así como el uso de ram es moderado.\n",
        "\n",
        "Incluye español latino y europeo, puede identificar acentos, ruidos de fondo y distintas velocidades de habla.\n",
        "\n",
        "La otra opción era Hugging Face como Wav2vec2 para español, sin embargo, requiere cierta configuración además de pyTorch"
      ],
      "metadata": {
        "id": "PpqdQxOSBZ1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "\n",
        "import whisper\n",
        "\n",
        "#modelo seleccionado\n",
        "modelo = whisper.load_model(\"small\")\n",
        "\n",
        "# Obtener rutas de audio válidas\n",
        "ruta_audios = [\n",
        "    os.path.join(DIR, f)\n",
        "    for f in os.listdir(DIR)\n",
        "    if f.endswith(\".mp3\")\n",
        "]\n",
        "\n",
        "transcripciones = {}\n",
        "\n",
        "#extracción de texto\n",
        "for ruta in ruta_audios:\n",
        "  nombre = os.path.basename(ruta)\n",
        "  identificador = os.path.splitext(nombre)[0]\n",
        "  resultado = modelo.transcribe(ruta, language=\"es\")\n",
        "  transcripciones[identificador] = resultado[\"text\"]\n",
        "\n",
        "for k in sorted(transcripciones.keys()):\n",
        "  print(f\"{k}: {transcripciones[k]}\")"
      ],
      "metadata": {
        "id": "C3k5sLGhnO1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4638c3-14ed-4508-f7dc-4bf069009cb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/usr/local/lib/python3.11/dist-packages/whisper/transcribe.py:126: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01:  Las fábulas de Sopo, grabado para LibriVox.org por Paulino, www.paulino.info. Fábula n. 61, el lobo y el cordero en el templo. Tándose cuenta de que era perseguido por un lobo, un pequeño cordelito decidió refugiarse en un templo cercano. Lo llamó el lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo emolaría a su Dios. Mejor así, replicó el cordero. Prefiero ser víctima para un Dios, a tener que perecer en tus colmillos. Si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor. Fin de la fábula. Esta es una grabación del dominio público.\n",
            "04:  Las fábulas de SOFO grabado para LibriVox.org por Roberto Antonio Muñoz, fábula número 64, el lobo y la grúda. A un lobo que comía un hueso, se le atragantó el hueso en la garganta y corría por todas partes en busca de auxilio. Encontró en su correr a una grulla y le pidió que le salvara de aquella situación y que enseguida le pagaría por ello. Aptó la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado. Pidió entonces la cancelación de la paga convenida. Oye, aniga, dijo el lobo, no crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? Nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendrías si te dejan sano y salvo. Fin de fábula. Esta grabación es de dominio público.\n",
            "05:  Las Fábulas de Sopo, grabado para LibriVox.org por Karen Savage. Fábula número 65. El Lobo y el Caballo. Pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dejó y siguió su camino. Encontró al rato a un caballo, y le llevó al campo comentándole la gran cantidad de cebada que había hallado, pero que en vez de comérsela él, mejor se la había dejado porque le agradaban más oír el ruido de sus dientes al masticarla. Pero el caballo le repuso, amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus oídos sino a tu estómago. A todo malvado, aunque parezca actuar como bueno, no debe de creérsela. Fin de Fábula. Esta grabación es de dominio público.\n",
            "06:  Las fábulas de Esopo, grabado para LibriVox.org por Alejandro González Calderón. Fábula número 66, el lobo y el asno. Un lobo fue elegido rey entre sus congéneres y decretó una ley ordenando que lo que cada uno capturase en la casa lo pusieran común y lo repartiese por partes iguales entre todos. De esta manera ya no tendrían los lobos que devorarse unos a otros en épocas de hambre. Pero en eso le escuchó un asno que estaba por ahí cerca y moviendo sus orejas le dijo magnífica idea abrotado de tu corazón, pero ¿por qué se escondido todo tu botín en tu cueva? Llevándola a la comunidad y repártelo también como lo ha decretado. El lobo descubrierte confundido derogó su ley. Si alguna vez llegas a tener poder de legislar, sé el primero en cumplir tus propias leyes. Fin de la fábula. Esta grabación es de dominio público.\n",
            "14:  Las Fábulas de Sopo, grabado para LibriVox.org por el ochito. Fábula número 74. El Lobo y el Cabrito Encerrado. Protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenzó a insultarle burlándose ampliamente de él. El lobo serenamente le replicó. Infeliz. Sé que no eres tú quien me está insultando sino el sitio en que te encuentras. Muy a menudo no es el valor sino la ocasión y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos. Fin de la Fábula. Esta grabación es del dominio público. www.fábulasdevox.org\n",
            "22:  Las Fábulas de Sopo, grabado para LibriVox.org por El Ochito Fábula número 82, el perro y la almeja Un perro de esos acostumbrados a comer huevos al ver una almeja no lo pensó dos veces y creyendo que se trataba de un huevo se la trago inmediatamente Desgarradas luego sus entrañas se sintió muy mal y se dijo, bien merecido lo tengo por creer que todo lo que veo redondo son huevos Nunca tomes un asunto antes reflexionar para no entrar luego en extrañas dificultades Fin de la Fábula Esta grabación es del dominio público Subtítulos por la comunidad de Amara.org\n",
            "24:  Las Fábulas de Sopo, grabado para LibriVox.org por Karen Zavic. Fábula número 84, El perro y el reflejo en el río. Badeaba a un perro un río, llevando en su hocico un sabroso pedazo de carne. Vio su propio reflejo en el agua del río y creyó que aquel reflejo era en realidad otro perro que llevaba un trozo de carne mayor que el suyo. Y deseando adueñarse del pedazo ajeno, soltó el suyo para arrebatar el trozo a su supuesto compadre. Pero el resultado fue que se quedó sin el propio y sin el ajeno. Este porque no existía, solo era un reflejo, y el otro, el verdadero, porque se lo llevó a la corriente. Nunca codices el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo. Fin de Fábula. Esta grabación es de dominio público.\n",
            "25:  Las fábulas de esopo grabada para LibreVox.org, fábula número 85, el perro y el carnicero. Penetró un perro en una carnicería y notando que el carnicero estaba muy ocupado con sus clientes cogió un trozo de carne y se lió corriendo. Se volvió el carnicero y viéndole huir y sin poder hacer y nada exclamó. Oye amigo, ahí donde te encuentre no dejaré de mirarte. No esperes a que suceda un accidente para pensar en cómo evitarlo. Fin de fábula. Esta grabación es de dominio público.\n",
            "26:  Las Fábulas de Esopo, grabado para LibriVox.org por el ochito Fábula número 86. El perro con campanilla Había un perro que acostumbraba a morder sin razón. Le puso su amo una campanilla para advertirle a la gente de su presencia cercana. Y el can, sonando la campanilla, se fue a la plaza pública a presumir. Más una sabia perra ya avanzada de años le dijo, ¿De qué presumes tanto amigo? Sé que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldado culta. Los halagos que se hacen a sí mismo los fanfarrones solo delatan sus mayores defectos. Fin de la Fábula. Esta grabación es del dominio público.\n",
            "27:  Las Fábulas de Sopo, grabado para LibriVox.org por El Ochito. Fábula número 87. El perro que perseguía al león. Un perro de casa se encontró con un león y partió en su persecución. Pero el león se volvió rugiendo y el perro, todo atemorizado, retrocedió rápidamente por el mismo camino. Le vio una zorra y le dijo, Perro infeliz, primero perseguías al león y ya ni siquiera soporta sus surgidos. Cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas. Fin de la Fábula. Esta grabación es del dominio público. Subtítulos por la comunidad de Amara.org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 2b:**"
      ],
      "metadata": {
        "id": "NM0D83j8EWiN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Eliminar el inicio y final comunes de los textos extraídos de cada fábula.**\n",
        "\n",
        "* #### **Sugerencia:** Pueden guardar esta información en un archivo tipo JSON, para que al estar probando diferentes opciones en los ejercicios siguientes, puedan recuperar rápidamente la información de cada video/fábula."
      ],
      "metadata": {
        "id": "xiFG5q88EYHU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "'''\n",
        "Se hace limpieza de inicio y fin común del texto extraido\n",
        "'''\n",
        "\n",
        "import re\n",
        "patron1 = r'fin de fábula'\n",
        "patron2 = r'fin de la fábula'\n",
        "fin = r'dominio público'\n",
        "transcrip_copia = transcripciones.copy()\n",
        "\n",
        "for id, texto in transcrip_copia.items():\n",
        "  patron = re.escape(\"las fábulas\") + r'.*?' + re.escape(\"fábula\") + r'.*?\\d+' #Patron para eliminar el inicio común.\n",
        "  patronFin = f'({re.escape(patron1)}|{re.escape(patron2)}).*?{re.escape(fin)}' #Patron para eliminar el fin común.\n",
        "  resultado = re.sub(r'^[\\s.,]+|[\\s.,]+$', '', re.sub(patronFin, '', re.sub(patron, '', texto.lower()))).strip() #Elimina el inicio y fin comunes. (.)(,)(\\s)\n",
        "\n",
        "  transcrip_copia[id] = resultado\n",
        "\n",
        "for k in sorted(transcrip_copia.keys()):\n",
        "  print(f\"{k}: {transcrip_copia[k]}\")"
      ],
      "metadata": {
        "id": "KkbeTmeon_RP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e86d55e-2c91-4fdf-c653-7133d51ace5c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "01: el lobo y el cordero en el templo. tándose cuenta de que era perseguido por un lobo, un pequeño cordelito decidió refugiarse en un templo cercano. lo llamó el lobo y le dijo que si el sacrificador lo encontraba allí adentro, lo emolaría a su dios. mejor así, replicó el cordero. prefiero ser víctima para un dios, a tener que perecer en tus colmillos. si sin remedio vamos a ser sacrificados, más nos vale que sea con el mayor honor\n",
            "04: el lobo y la grúda. a un lobo que comía un hueso, se le atragantó el hueso en la garganta y corría por todas partes en busca de auxilio. encontró en su correr a una grulla y le pidió que le salvara de aquella situación y que enseguida le pagaría por ello. aptó la grulla e introdujo su cabeza en la boca del lobo, sacando de la garganta el hueso atravesado. pidió entonces la cancelación de la paga convenida. oye, aniga, dijo el lobo, no crees que es suficiente paga con haber sacado tu cabeza sana y salva de mi boca? nunca hagas favores a malvados, traficantes o corruptos, pues mucha paga tendrías si te dejan sano y salvo\n",
            "05: el lobo y el caballo. pasaba un lobo por un sembrado de cebada, pero como no era comida de su gusto, la dejó y siguió su camino. encontró al rato a un caballo, y le llevó al campo comentándole la gran cantidad de cebada que había hallado, pero que en vez de comérsela él, mejor se la había dejado porque le agradaban más oír el ruido de sus dientes al masticarla. pero el caballo le repuso, amigo, si los lobos comieran cebada, no hubieras preferido complacer a tus oídos sino a tu estómago. a todo malvado, aunque parezca actuar como bueno, no debe de creérsela\n",
            "06: el lobo y el asno. un lobo fue elegido rey entre sus congéneres y decretó una ley ordenando que lo que cada uno capturase en la casa lo pusieran común y lo repartiese por partes iguales entre todos. de esta manera ya no tendrían los lobos que devorarse unos a otros en épocas de hambre. pero en eso le escuchó un asno que estaba por ahí cerca y moviendo sus orejas le dijo magnífica idea abrotado de tu corazón, pero ¿por qué se escondido todo tu botín en tu cueva? llevándola a la comunidad y repártelo también como lo ha decretado. el lobo descubrierte confundido derogó su ley. si alguna vez llegas a tener poder de legislar, sé el primero en cumplir tus propias leyes\n",
            "14: el lobo y el cabrito encerrado. protegido por la seguridad del corral de una casa, un cabrito vio pasar a un lobo y comenzó a insultarle burlándose ampliamente de él. el lobo serenamente le replicó. infeliz. sé que no eres tú quien me está insultando sino el sitio en que te encuentras. muy a menudo no es el valor sino la ocasión y el lugar quienes proveen el enfrentamiento arrogante ante los poderosos. . www.fábulasdevox.org\n",
            "22: el perro y la almeja un perro de esos acostumbrados a comer huevos al ver una almeja no lo pensó dos veces y creyendo que se trataba de un huevo se la trago inmediatamente desgarradas luego sus entrañas se sintió muy mal y se dijo, bien merecido lo tengo por creer que todo lo que veo redondo son huevos nunca tomes un asunto antes reflexionar para no entrar luego en extrañas dificultades  subtítulos por la comunidad de amara.org\n",
            "24: el perro y el reflejo en el río. badeaba a un perro un río, llevando en su hocico un sabroso pedazo de carne. vio su propio reflejo en el agua del río y creyó que aquel reflejo era en realidad otro perro que llevaba un trozo de carne mayor que el suyo. y deseando adueñarse del pedazo ajeno, soltó el suyo para arrebatar el trozo a su supuesto compadre. pero el resultado fue que se quedó sin el propio y sin el ajeno. este porque no existía, solo era un reflejo, y el otro, el verdadero, porque se lo llevó a la corriente. nunca codices el bien ajeno, pues puedes perder lo que ya has adquirido con tu esfuerzo\n",
            "25: el perro y el carnicero. penetró un perro en una carnicería y notando que el carnicero estaba muy ocupado con sus clientes cogió un trozo de carne y se lió corriendo. se volvió el carnicero y viéndole huir y sin poder hacer y nada exclamó. oye amigo, ahí donde te encuentre no dejaré de mirarte. no esperes a que suceda un accidente para pensar en cómo evitarlo\n",
            "26: el perro con campanilla había un perro que acostumbraba a morder sin razón. le puso su amo una campanilla para advertirle a la gente de su presencia cercana. y el can, sonando la campanilla, se fue a la plaza pública a presumir. más una sabia perra ya avanzada de años le dijo, ¿de qué presumes tanto amigo? sé que no llevas esa campanilla por tus grandes virtudes, sino para anunciar tu maldado culta. los halagos que se hacen a sí mismo los fanfarrones solo delatan sus mayores defectos\n",
            "27: el perro que perseguía al león. un perro de casa se encontró con un león y partió en su persecución. pero el león se volvió rugiendo y el perro, todo atemorizado, retrocedió rápidamente por el mismo camino. le vio una zorra y le dijo, perro infeliz, primero perseguías al león y ya ni siquiera soporta sus surgidos. cuando entres a una empresa, mantente siempre listo a afrontar imprevistos que no te imaginabas. . subtítulos por la comunidad de amara.org\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 3:**"
      ],
      "metadata": {
        "id": "6PKaB_Ge0Shc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Apliquen el proceso de limpieza que consideren adecuado.**\n",
        "\n",
        "* #### **Justifiquen los pasos de limpieza utilizados. Tomen en cuenta que el texto extraído de cada fábula es relativamente pequeño.**\n",
        "\n",
        "* #### **En caso de que decidan no aplicar esta etapa de limpieza, deberán justificarlo.**"
      ],
      "metadata": {
        "id": "zNrqcQFe0VWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Antes de aplicar el modelo LDA (Latent Dirichlet Allocation) para la extracción de temas en textos, es fundamental realizar un proceso de limpieza lingüística. Este paso mejora significativamente la calidad del análisis, ya que LDA es sensible al ruido textual. En la función limpiar_texto, primero se convierten todos los caracteres a minúsculas para evitar duplicidad de palabras con distintas capitalizaciones. Luego, se eliminan los números y signos de puntuación, ya que no aportan información semántica relevante al modelo.\n",
        "\n",
        "Posteriormente, se eliminan las palabras vacías (stopwords) del español, que son términos comunes sin carga temática (como “el”, “y”, “de”), y se descartan palabras demasiado cortas, que suelen ser poco informativas. Este preprocesamiento asegura que el modelo LDA se concentre en las palabras más representativas y temáticamente significativas del texto, facilitando la identificación de tópicos coherente"
      ],
      "metadata": {
        "id": "KnjeP31xfIqs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "'''\n",
        "Se aplica limpieza mediante stopwords, se trabaja con minusculas, y solo palabras de texto y números\n",
        "'''\n",
        "stopwords = ['el','la', 'con', 'a', 'de', 'los', 'las', 'un', 'una', 'lo','al','y','se','le','en','su','que','tu','tus','sus','no','uno','o',\n",
        "             'www','fábulasdevox','org','subtitulos', 'subtítulos','si','sino','ya','por','pero','eso','se','sé','te','tu','me','muy','tu','tú',\n",
        "             'si','sí','mas','más','que','qué','son']\n",
        "\n",
        "def limpieza(text, stopw):\n",
        "  text = text.lower()\n",
        "  tokens = re.findall(r'\\b\\w+\\b', text)\n",
        "  tokens_limpios = [palabra for palabra in tokens if palabra not in stopw]\n",
        "  return tokens_limpios\n",
        "\n",
        "fabulas_limpias = []\n",
        "audio_text_clean = {}\n",
        "\n",
        "for i, t in transcrip_copia.items():\n",
        "  tokens = limpieza(t, stopwords)\n",
        "  fabulas_limpias.append(tokens)\n",
        "  audio_text_clean[i] = tokens\n",
        "\n",
        "for i, text in enumerate(fabulas_limpias, 1):\n",
        "  print(text)"
      ],
      "metadata": {
        "id": "pqwiCCdpq8D_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "342b1ea9-480c-4157-88e3-e13ab55cc7be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['lobo', 'cordero', 'templo', 'tándose', 'cuenta', 'era', 'perseguido', 'lobo', 'pequeño', 'cordelito', 'decidió', 'refugiarse', 'templo', 'cercano', 'llamó', 'lobo', 'dijo', 'sacrificador', 'encontraba', 'allí', 'adentro', 'emolaría', 'dios', 'mejor', 'así', 'replicó', 'cordero', 'prefiero', 'ser', 'víctima', 'para', 'dios', 'tener', 'perecer', 'colmillos', 'sin', 'remedio', 'vamos', 'ser', 'sacrificados', 'nos', 'vale', 'sea', 'mayor', 'honor']\n",
            "['lobo', 'grúda', 'lobo', 'comía', 'hueso', 'atragantó', 'hueso', 'garganta', 'corría', 'todas', 'partes', 'busca', 'auxilio', 'encontró', 'correr', 'grulla', 'pidió', 'salvara', 'aquella', 'situación', 'enseguida', 'pagaría', 'ello', 'aptó', 'grulla', 'e', 'introdujo', 'cabeza', 'boca', 'del', 'lobo', 'sacando', 'garganta', 'hueso', 'atravesado', 'pidió', 'entonces', 'cancelación', 'paga', 'convenida', 'oye', 'aniga', 'dijo', 'lobo', 'crees', 'es', 'suficiente', 'paga', 'haber', 'sacado', 'cabeza', 'sana', 'salva', 'mi', 'boca', 'nunca', 'hagas', 'favores', 'malvados', 'traficantes', 'corruptos', 'pues', 'mucha', 'paga', 'tendrías', 'dejan', 'sano', 'salvo']\n",
            "['lobo', 'caballo', 'pasaba', 'lobo', 'sembrado', 'cebada', 'como', 'era', 'comida', 'gusto', 'dejó', 'siguió', 'camino', 'encontró', 'rato', 'caballo', 'llevó', 'campo', 'comentándole', 'gran', 'cantidad', 'cebada', 'había', 'hallado', 'vez', 'comérsela', 'él', 'mejor', 'había', 'dejado', 'porque', 'agradaban', 'oír', 'ruido', 'dientes', 'masticarla', 'caballo', 'repuso', 'amigo', 'lobos', 'comieran', 'cebada', 'hubieras', 'preferido', 'complacer', 'oídos', 'estómago', 'todo', 'malvado', 'aunque', 'parezca', 'actuar', 'como', 'bueno', 'debe', 'creérsela']\n",
            "['lobo', 'asno', 'lobo', 'fue', 'elegido', 'rey', 'entre', 'congéneres', 'decretó', 'ley', 'ordenando', 'cada', 'capturase', 'casa', 'pusieran', 'común', 'repartiese', 'partes', 'iguales', 'entre', 'todos', 'esta', 'manera', 'tendrían', 'lobos', 'devorarse', 'unos', 'otros', 'épocas', 'hambre', 'escuchó', 'asno', 'estaba', 'ahí', 'cerca', 'moviendo', 'orejas', 'dijo', 'magnífica', 'idea', 'abrotado', 'corazón', 'escondido', 'todo', 'botín', 'cueva', 'llevándola', 'comunidad', 'repártelo', 'también', 'como', 'ha', 'decretado', 'lobo', 'descubrierte', 'confundido', 'derogó', 'ley', 'alguna', 'vez', 'llegas', 'tener', 'poder', 'legislar', 'primero', 'cumplir', 'propias', 'leyes']\n",
            "['lobo', 'cabrito', 'encerrado', 'protegido', 'seguridad', 'del', 'corral', 'casa', 'cabrito', 'vio', 'pasar', 'lobo', 'comenzó', 'insultarle', 'burlándose', 'ampliamente', 'él', 'lobo', 'serenamente', 'replicó', 'infeliz', 'eres', 'quien', 'está', 'insultando', 'sitio', 'encuentras', 'menudo', 'es', 'valor', 'ocasión', 'lugar', 'quienes', 'proveen', 'enfrentamiento', 'arrogante', 'ante', 'poderosos']\n",
            "['perro', 'almeja', 'perro', 'esos', 'acostumbrados', 'comer', 'huevos', 'ver', 'almeja', 'pensó', 'dos', 'veces', 'creyendo', 'trataba', 'huevo', 'trago', 'inmediatamente', 'desgarradas', 'luego', 'entrañas', 'sintió', 'mal', 'dijo', 'bien', 'merecido', 'tengo', 'creer', 'todo', 'veo', 'redondo', 'huevos', 'nunca', 'tomes', 'asunto', 'antes', 'reflexionar', 'para', 'entrar', 'luego', 'extrañas', 'dificultades', 'comunidad', 'amara']\n",
            "['perro', 'reflejo', 'río', 'badeaba', 'perro', 'río', 'llevando', 'hocico', 'sabroso', 'pedazo', 'carne', 'vio', 'propio', 'reflejo', 'agua', 'del', 'río', 'creyó', 'aquel', 'reflejo', 'era', 'realidad', 'otro', 'perro', 'llevaba', 'trozo', 'carne', 'mayor', 'suyo', 'deseando', 'adueñarse', 'del', 'pedazo', 'ajeno', 'soltó', 'suyo', 'para', 'arrebatar', 'trozo', 'supuesto', 'compadre', 'resultado', 'fue', 'quedó', 'sin', 'propio', 'sin', 'ajeno', 'este', 'porque', 'existía', 'solo', 'era', 'reflejo', 'otro', 'verdadero', 'porque', 'llevó', 'corriente', 'nunca', 'codices', 'bien', 'ajeno', 'pues', 'puedes', 'perder', 'has', 'adquirido', 'esfuerzo']\n",
            "['perro', 'carnicero', 'penetró', 'perro', 'carnicería', 'notando', 'carnicero', 'estaba', 'ocupado', 'clientes', 'cogió', 'trozo', 'carne', 'lió', 'corriendo', 'volvió', 'carnicero', 'viéndole', 'huir', 'sin', 'poder', 'hacer', 'nada', 'exclamó', 'oye', 'amigo', 'ahí', 'donde', 'encuentre', 'dejaré', 'mirarte', 'esperes', 'suceda', 'accidente', 'para', 'pensar', 'cómo', 'evitarlo']\n",
            "['perro', 'campanilla', 'había', 'perro', 'acostumbraba', 'morder', 'sin', 'razón', 'puso', 'amo', 'campanilla', 'para', 'advertirle', 'gente', 'presencia', 'cercana', 'can', 'sonando', 'campanilla', 'fue', 'plaza', 'pública', 'presumir', 'sabia', 'perra', 'avanzada', 'años', 'dijo', 'presumes', 'tanto', 'amigo', 'llevas', 'esa', 'campanilla', 'grandes', 'virtudes', 'para', 'anunciar', 'maldado', 'culta', 'halagos', 'hacen', 'mismo', 'fanfarrones', 'solo', 'delatan', 'mayores', 'defectos']\n",
            "['perro', 'perseguía', 'león', 'perro', 'casa', 'encontró', 'león', 'partió', 'persecución', 'león', 'volvió', 'rugiendo', 'perro', 'todo', 'atemorizado', 'retrocedió', 'rápidamente', 'mismo', 'camino', 'vio', 'zorra', 'dijo', 'perro', 'infeliz', 'primero', 'perseguías', 'león', 'ni', 'siquiera', 'soporta', 'surgidos', 'cuando', 'entres', 'empresa', 'mantente', 'siempre', 'listo', 'afrontar', 'imprevistos', 'imaginabas', 'comunidad', 'amara']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(audio_text_clean)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olLcX8EWvKqI",
        "outputId": "1e15715d-45cc-4072-8d85-ed5a6f706f5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'01': ['lobo', 'cordero', 'templo', 'tándose', 'cuenta', 'era', 'perseguido', 'lobo', 'pequeño', 'cordelito', 'decidió', 'refugiarse', 'templo', 'cercano', 'llamó', 'lobo', 'dijo', 'sacrificador', 'encontraba', 'allí', 'adentro', 'emolaría', 'dios', 'mejor', 'así', 'replicó', 'cordero', 'prefiero', 'ser', 'víctima', 'para', 'dios', 'tener', 'perecer', 'colmillos', 'sin', 'remedio', 'vamos', 'ser', 'sacrificados', 'nos', 'vale', 'sea', 'mayor', 'honor'], '04': ['lobo', 'grúda', 'lobo', 'comía', 'hueso', 'atragantó', 'hueso', 'garganta', 'corría', 'todas', 'partes', 'busca', 'auxilio', 'encontró', 'correr', 'grulla', 'pidió', 'salvara', 'aquella', 'situación', 'enseguida', 'pagaría', 'ello', 'aptó', 'grulla', 'e', 'introdujo', 'cabeza', 'boca', 'del', 'lobo', 'sacando', 'garganta', 'hueso', 'atravesado', 'pidió', 'entonces', 'cancelación', 'paga', 'convenida', 'oye', 'aniga', 'dijo', 'lobo', 'crees', 'es', 'suficiente', 'paga', 'haber', 'sacado', 'cabeza', 'sana', 'salva', 'mi', 'boca', 'nunca', 'hagas', 'favores', 'malvados', 'traficantes', 'corruptos', 'pues', 'mucha', 'paga', 'tendrías', 'dejan', 'sano', 'salvo'], '05': ['lobo', 'caballo', 'pasaba', 'lobo', 'sembrado', 'cebada', 'como', 'era', 'comida', 'gusto', 'dejó', 'siguió', 'camino', 'encontró', 'rato', 'caballo', 'llevó', 'campo', 'comentándole', 'gran', 'cantidad', 'cebada', 'había', 'hallado', 'vez', 'comérsela', 'él', 'mejor', 'había', 'dejado', 'porque', 'agradaban', 'oír', 'ruido', 'dientes', 'masticarla', 'caballo', 'repuso', 'amigo', 'lobos', 'comieran', 'cebada', 'hubieras', 'preferido', 'complacer', 'oídos', 'estómago', 'todo', 'malvado', 'aunque', 'parezca', 'actuar', 'como', 'bueno', 'debe', 'creérsela'], '06': ['lobo', 'asno', 'lobo', 'fue', 'elegido', 'rey', 'entre', 'congéneres', 'decretó', 'ley', 'ordenando', 'cada', 'capturase', 'casa', 'pusieran', 'común', 'repartiese', 'partes', 'iguales', 'entre', 'todos', 'esta', 'manera', 'tendrían', 'lobos', 'devorarse', 'unos', 'otros', 'épocas', 'hambre', 'escuchó', 'asno', 'estaba', 'ahí', 'cerca', 'moviendo', 'orejas', 'dijo', 'magnífica', 'idea', 'abrotado', 'corazón', 'escondido', 'todo', 'botín', 'cueva', 'llevándola', 'comunidad', 'repártelo', 'también', 'como', 'ha', 'decretado', 'lobo', 'descubrierte', 'confundido', 'derogó', 'ley', 'alguna', 'vez', 'llegas', 'tener', 'poder', 'legislar', 'primero', 'cumplir', 'propias', 'leyes'], '14': ['lobo', 'cabrito', 'encerrado', 'protegido', 'seguridad', 'del', 'corral', 'casa', 'cabrito', 'vio', 'pasar', 'lobo', 'comenzó', 'insultarle', 'burlándose', 'ampliamente', 'él', 'lobo', 'serenamente', 'replicó', 'infeliz', 'eres', 'quien', 'está', 'insultando', 'sitio', 'encuentras', 'menudo', 'es', 'valor', 'ocasión', 'lugar', 'quienes', 'proveen', 'enfrentamiento', 'arrogante', 'ante', 'poderosos'], '22': ['perro', 'almeja', 'perro', 'esos', 'acostumbrados', 'comer', 'huevos', 'ver', 'almeja', 'pensó', 'dos', 'veces', 'creyendo', 'trataba', 'huevo', 'trago', 'inmediatamente', 'desgarradas', 'luego', 'entrañas', 'sintió', 'mal', 'dijo', 'bien', 'merecido', 'tengo', 'creer', 'todo', 'veo', 'redondo', 'huevos', 'nunca', 'tomes', 'asunto', 'antes', 'reflexionar', 'para', 'entrar', 'luego', 'extrañas', 'dificultades', 'comunidad', 'amara'], '24': ['perro', 'reflejo', 'río', 'badeaba', 'perro', 'río', 'llevando', 'hocico', 'sabroso', 'pedazo', 'carne', 'vio', 'propio', 'reflejo', 'agua', 'del', 'río', 'creyó', 'aquel', 'reflejo', 'era', 'realidad', 'otro', 'perro', 'llevaba', 'trozo', 'carne', 'mayor', 'suyo', 'deseando', 'adueñarse', 'del', 'pedazo', 'ajeno', 'soltó', 'suyo', 'para', 'arrebatar', 'trozo', 'supuesto', 'compadre', 'resultado', 'fue', 'quedó', 'sin', 'propio', 'sin', 'ajeno', 'este', 'porque', 'existía', 'solo', 'era', 'reflejo', 'otro', 'verdadero', 'porque', 'llevó', 'corriente', 'nunca', 'codices', 'bien', 'ajeno', 'pues', 'puedes', 'perder', 'has', 'adquirido', 'esfuerzo'], '25': ['perro', 'carnicero', 'penetró', 'perro', 'carnicería', 'notando', 'carnicero', 'estaba', 'ocupado', 'clientes', 'cogió', 'trozo', 'carne', 'lió', 'corriendo', 'volvió', 'carnicero', 'viéndole', 'huir', 'sin', 'poder', 'hacer', 'nada', 'exclamó', 'oye', 'amigo', 'ahí', 'donde', 'encuentre', 'dejaré', 'mirarte', 'esperes', 'suceda', 'accidente', 'para', 'pensar', 'cómo', 'evitarlo'], '26': ['perro', 'campanilla', 'había', 'perro', 'acostumbraba', 'morder', 'sin', 'razón', 'puso', 'amo', 'campanilla', 'para', 'advertirle', 'gente', 'presencia', 'cercana', 'can', 'sonando', 'campanilla', 'fue', 'plaza', 'pública', 'presumir', 'sabia', 'perra', 'avanzada', 'años', 'dijo', 'presumes', 'tanto', 'amigo', 'llevas', 'esa', 'campanilla', 'grandes', 'virtudes', 'para', 'anunciar', 'maldado', 'culta', 'halagos', 'hacen', 'mismo', 'fanfarrones', 'solo', 'delatan', 'mayores', 'defectos'], '27': ['perro', 'perseguía', 'león', 'perro', 'casa', 'encontró', 'león', 'partió', 'persecución', 'león', 'volvió', 'rugiendo', 'perro', 'todo', 'atemorizado', 'retrocedió', 'rápidamente', 'mismo', 'camino', 'vio', 'zorra', 'dijo', 'perro', 'infeliz', 'primero', 'perseguías', 'león', 'ni', 'siquiera', 'soporta', 'surgidos', 'cuando', 'entres', 'empresa', 'mantente', 'siempre', 'listo', 'afrontar', 'imprevistos', 'imaginabas', 'comunidad', 'amara']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 4:**"
      ],
      "metadata": {
        "id": "i2ywrmsMP_EF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def palabras_clave_lda(fabulas_tokenizadas, num_palabras=25):\n",
        "    \"\"\"\n",
        "    Aplica LDA a cada fábula por separado para extraer palabras clave.\n",
        "    Cada fábula se asume como un solo tópico.\n",
        "\n",
        "    Parámetros\n",
        "    ----------\n",
        "    fabulas_tokenizadas : dict\n",
        "        {nombre_audio: [tokens]}\n",
        "    num_palabras : int\n",
        "        número de palabras clave por tópico\n",
        "\n",
        "    Retorna\n",
        "    -------\n",
        "    resultados : dict\n",
        "        Diccionario con las palabras clave por cada fábula\n",
        "    \"\"\"\n",
        "    resultados = {}\n",
        "\n",
        "    for nombre, tokens in fabulas_tokenizadas.items():\n",
        "        # Crear diccionario y corpus para una sola fábula\n",
        "        diccionario = corpora.Dictionary([tokens])\n",
        "        corpus = [diccionario.doc2bow(tokens)]\n",
        "\n",
        "        # Entrenar LDA para 1 tópico\n",
        "        lda = LdaModel(corpus, num_topics=1, id2word=diccionario, passes=10)\n",
        "\n",
        "        # Obtener las palabras clave del único tópico\n",
        "        palabras_clave = lda.show_topic(0, topn=num_palabras)\n",
        "        solo_palabras = [palabra for palabra, _ in palabras_clave]\n",
        "\n",
        "        resultados[nombre] = solo_palabras\n",
        "\n",
        "    return resultados"
      ],
      "metadata": {
        "id": "uez0HHO-kZPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Desplegando palabras clave para cafa fábula\n",
        "fabulas_lda = palabras_clave_lda(audio_text_clean, num_palabras=25)\n",
        "fabulas_ldaStr = {}\n",
        "\n",
        "for nombre, palabras in fabulas_lda.items():\n",
        "    fabulas_ldaStr[nombre] = ' '.join(palabras)\n",
        "    print(f\"\\n{nombre}:\\n{', '.join(palabras)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_X51IOblLy3",
        "outputId": "5dad6fdb-ca92-4b66-af7c-4c82a0ce63e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "01:\n",
            "lobo, dios, cordero, templo, ser, perecer, cordelito, pequeño, adentro, perseguido, prefiero, refugiarse, remedio, replicó, sacrificados, cercano, sea, sin, tener, tándose, vale, vamos, nos, sacrificador, cuenta\n",
            "\n",
            "04:\n",
            "lobo, paga, hueso, garganta, boca, cabeza, grulla, pidió, partes, situación, suficiente, todas, sana, sano, pagaría, oye, sacando, nunca, mucha, mi, malvados, introdujo, pues, haber, salva\n",
            "\n",
            "05:\n",
            "caballo, cebada, había, lobo, como, parezca, rato, preferido, pasaba, siguió, todo, vez, ruido, sembrado, oír, gran, oídos, mejor, masticarla, malvado, lobos, llevó, hubieras, porque, repuso\n",
            "\n",
            "06:\n",
            "lobo, asno, ley, entre, partes, poder, todo, todos, unos, vez, también, tener, ahí, otros, ordenando, tendrían, moviendo, magnífica, lobos, llevándola, llegas, legislar, primero, orejas, propias\n",
            "\n",
            "14:\n",
            "lobo, cabrito, del, comenzó, burlándose, casa, ampliamente, lugar, menudo, ocasión, pasar, poderosos, protegido, proveen, quien, quienes, replicó, seguridad, serenamente, sitio, valor, vio, insultarle, arrogante, encuentras\n",
            "\n",
            "22:\n",
            "luego, almeja, perro, huevos, tengo, huevo, veo, veces, antes, bien, acostumbrados, mal, merecido, nunca, pensó, comer, redondo, reflexionar, sintió, todo, tomes, trago, trataba, para, creer\n",
            "\n",
            "24:\n",
            "reflejo, perro, ajeno, río, otro, suyo, porque, era, sin, pedazo, propio, carne, del, trozo, realidad, resultado, puedes, sabroso, soltó, perder, verdadero, pues, llevó, supuesto, solo\n",
            "\n",
            "25:\n",
            "carnicero, perro, lió, corriendo, clientes, carne, carnicería, huir, accidente, mirarte, nada, notando, ocupado, oye, para, penetró, pensar, poder, sin, suceda, trozo, viéndole, hacer, amigo, dejaré\n",
            "\n",
            "26:\n",
            "campanilla, perro, para, cercana, amo, can, acostumbraba, advertirle, maldado, mismo, morder, perra, hacen, presencia, presumes, presumir, puso, pública, razón, solo, sonando, tanto, halagos, grandes, avanzada\n",
            "\n",
            "27:\n",
            "perro, león, partió, empresa, cuando, camino, comunidad, afrontar, persecución, perseguía, perseguías, casa, primero, rugiendo, rápidamente, siempre, siquiera, soporta, surgidos, todo, vio, volvió, mismo, retrocedió, atemorizado\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 5a y 5b:**"
      ],
      "metadata": {
        "id": "Blrrs1sWwkSx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **5a: Mediante el LLM que hayan seleccionado, generar un único enunciado que describa o resuma cada fábula.**\n",
        "\n",
        "* #### **5b: Mediante el LLM que hayan seleccionado, generar tres posibles enunciados diferentes relacionados con la historia de la fábula.**\n",
        "\n",
        "* #### **Sugerencia:** En realidad los dos incisos a y b se pueden obtener con un solo prompt que solicite la información y el formato correspondiente para cada una de estas partes. Por ejemplo, para cada fábula la salida puede ser un primer enunciado genérico que resume o describe dicha temática; seguido de tres enunciados, cada uno hablando sobre una situación o parte diferente de la fábula."
      ],
      "metadata": {
        "id": "gWvzQ-aNwsVk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Incluyan a continuación todas las celdas (de código o texto) que deseen...\n",
        "!pip install openai\n",
        "\n",
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "import openai"
      ],
      "metadata": {
        "id": "Q9UkVPxM0Xii",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "d404b262-ad2c-4e21-fc30-43f29cd8536f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.84.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.5)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.14.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "api_key = userdata.get(\"POM_TU_API_KEY_AQUI\")\n",
        "client = OpenAI(api_key = api_key)"
      ],
      "metadata": {
        "id": "4twENTpd0hru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para hacer request a la API de OpenAI via Chat Completions\n",
        "def complete(system_prompt,user_prompt):\n",
        "    completion = client.chat.completions.create(\n",
        "        model='gpt-4.1',\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_prompt},\n",
        "        ]\n",
        "    )\n",
        "    return completion.choices[0].message.content"
      ],
      "metadata": {
        "id": "KY0baYq_z1hU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SYSTEM PROMPT\n",
        "sys_prompt = \"\"\"\n",
        "Eres un asistente experto en literatura y narrativa breve. Tu tarea es analizar una fábula y generar una lista de Python con exactamente 4 elementos:\n",
        "\n",
        "- Un enunciado que resuma de forma clara y concisa el mensaje principal de la fábula (moraleja).\n",
        "\n",
        "- Tres enunciados adicionales que representen posibles subtemas alternativos o complementarios al mensaje principal.\n",
        "\n",
        "Devuelve únicamente una lista de Python con los 4 enunciados en este orden:\n",
        "\n",
        "[\"Resumen principal de la fábula.\",\"Subtema 1\",\"Subtema 2\",\"Subtema 3\"]\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "Uhg6NdCj0E61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creando diccionario para almacenar los resultados\n",
        "llm_summary = {}\n",
        "\n",
        "# Obteniendo resultado para cada una de las fábulas\n",
        "for nombre, fabula in fabulas_ldaStr.items():\n",
        "    llm_summary[nombre] = complete(sys_prompt,fabula)\n",
        "    print(f'Fabula {nombre} ha sido resumida exitosamente.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ADZhjE370HZV",
        "outputId": "770607bb-045c-4dc5-d22d-c3ec8c035b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fabula 01 ha sido resumida exitosamente.\n",
            "Fabula 04 ha sido resumida exitosamente.\n",
            "Fabula 05 ha sido resumida exitosamente.\n",
            "Fabula 06 ha sido resumida exitosamente.\n",
            "Fabula 14 ha sido resumida exitosamente.\n",
            "Fabula 22 ha sido resumida exitosamente.\n",
            "Fabula 24 ha sido resumida exitosamente.\n",
            "Fabula 25 ha sido resumida exitosamente.\n",
            "Fabula 26 ha sido resumida exitosamente.\n",
            "Fabula 27 ha sido resumida exitosamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "llm_summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_cU-jESDTU9",
        "outputId": "f3612a89-d573-4ed0-84ca-7f7a79a51d7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'01': '[\"Es mejor afrontar los problemas directamente que buscar remedios que puedan ser peores que el mal original.\",\"La prudencia al escoger dónde buscar ayuda puede salvarnos de mayores peligros.\",\"Las apariencias no siempre garantizan seguridad; lo que parece refugio puede ser una trampa.\",\"Es importante pensar en las consecuencias a largo plazo antes de tomar decisiones apresuradas.\"]',\n",
              " '04': '[\"No esperes gratitud ni recompensa de los malvados cuando solo has hecho lo necesario para salir de peligro.\",\"Ayudar a un ser perjudicial puede ponerte en peligro sin garantía de reconocimiento.\",\"La prudencia al escoger a quién ayudar es fundamental para evitar daños innecesarios.\",\"El bienestar propio debe valorarse antes que la expectativa de recompensa.\"]',\n",
              " '05': '[\"La prudencia y la vigilancia pueden salvarnos de los peligros ocultos.\", \"No debemos distraernos con placeres inmediatos si hay amenazas cerca.\", \"La astucia puede ayudarnos a escapar de situaciones peligrosas.\", \"No siempre lo que parece apetecible es seguro.\"]',\n",
              " '06': '[\"No es sabio que los poderosos legislen solo en beneficio propio.\", \"El abuso de poder puede llevar a la injusticia entre los grupos.\", \"Todos deben tener voz en la creación de normas para que sean justas.\", \"El egoísmo de quienes mandan puede perjudicar el bien común.\"]',\n",
              " '14': '[\"La seguridad protege mejor que el valor temerario contra los peligros.\", \"No es sensato burlarse de los poderosos cuando se está en posición segura.\", \"La protección y el refugio pueden marcar la diferencia frente a las amenazas externas.\", \"La prudencia y la reflexión superan a la imprudencia y la arrogancia.\"]',\n",
              " '22': '[\"No debes juzgar las cosas por las apariencias o suposiciones.\", \"La costumbre puede llevarnos a cometer errores por no reflexionar adecuadamente.\", \"A veces recibimos lo que merecemos como consecuencia de nuestros propios actos.\", \"Es importante detenerse a pensar antes de actuar o tomar decisiones.\"]',\n",
              " '24': '[\"La avaricia puede llevarnos a perder lo que ya poseemos por desear lo ajeno.\",\"Las apariencias pueden engañar y hacer que tomemos malas decisiones.\",\"La falta de conformidad nos impide valorar lo que realmente tenemos.\",\"La precipitación en actuar sin pensar puede causar resultados negativos.\"]',\n",
              " '25': '[\"La fábula enseña que la desconfianza excesiva puede llevarnos a perder oportunidades valiosas.\", \"La obsesión por vigilar puede distraernos de asuntos verdaderamente importantes.\", \"Juzgar a otros precipitadamente puede hacernos cometer errores.\", \"Descuidar nuestras responsabilidades por sospechas infundadas trae consecuencias negativas.\"]',\n",
              " '26': '[\"La verdadera valía no reside en la apariencia ni en el ruido que hacemos, sino en nuestras acciones.\", \"La vanidad puede llevar a la confusión entre apariencia y sustancia.\", \"No todos los que alardean poseen realmente grandes cualidades.\", \"La ostentación pública a menudo oculta una falta de mérito real.\"]',\n",
              " '27': '[\"No debes emprender empresas para las que no tienes fortaleza o ánimo suficiente.\",\"El miedo puede hacer retroceder incluso a quienes parecen valientes.\",\"Es importante conocerse a uno mismo antes de enfrentarse a grandes retos.\",\"La apariencia de valentía no siempre corresponde a verdadera determinación.\"]'}"
            ]
          },
          "metadata": {},
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ejercicio 6:**"
      ],
      "metadata": {
        "id": "Kx-dZSFJz9cK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* #### **Incluyan sus conclusiones de la actividad audio-a-texto:**\n",
        "\n",
        "\n",
        "\n",
        "Los mayores problemas a los que nos enfrentamos fue en cuestion de las bibliotecas y las versiones correspondientes por dependencias entre sí.\n",
        "\n",
        "De forma adicional en textos tan cortos resulta complicado decidir cómo y que implementar de limpieza para lograr un resultado optimo sin impactar la interpretación del texto por el LLM y que no se pierda contexto.\n",
        "\n",
        "Sin embargo se puede apreciar la potencia de las herramientas tanto en transcripción de audio-texto, así como también en la interpretación y resultados obtenidos con base en el prompt alimentado y sus palabras clave."
      ],
      "metadata": {
        "id": "3w3usdaC0BCj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los resultados de esta actividad demuestran con claridad el rendimiento superior del modelo openai/whisper-1 en tareas de transcripción automática. Su capacidad para preservar la ortografía, puntuación y segmentación del discurso lo posiciona como una herramienta altamente confiable para aplicaciones profesionales en procesamiento de audio. A diferencia de otros modelos evaluados, Whisper-1 mantiene la fidelidad semántica del mensaje original, incluso en los detalles contextuales más sutiles. Esto es especialmente relevante cuando se busca preservar la intención narrativa de textos como fábulas, donde los matices lingüísticos y morales deben conservarse con precisión.\n",
        "\n",
        "Además, el análisis temático mediante LDA y la posterior interpretación generada con un modelo de lenguaje permitió extraer el núcleo narrativo de cada audio y elaborar subtemas significativos. Estos subtemas revelan patrones morales y estructurales recurrentes en las fábulas, como la lucha entre fuerza y astucia, el aprendizaje a través del error, o la justicia en contextos simbólicos. Esta combinación de herramientas no solo facilita la clasificación y comprensión automatizada de contenidos orales, sino que también abre posibilidades para el desarrollo de sistemas educativos y de análisis literario apoyados por inteligencia artificial."
      ],
      "metadata": {
        "id": "KXTlFpZefSvy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Fin de la actividad LDA y LMM: audio-a-texto**"
      ],
      "metadata": {
        "id": "CtB5Q3m41YQ0"
      }
    }
  ]
}